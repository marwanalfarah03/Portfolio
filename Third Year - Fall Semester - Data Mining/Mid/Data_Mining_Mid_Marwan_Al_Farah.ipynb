{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lClMx8d3Ime4"
   },
   "source": [
    "<br>\n",
    "\n",
    "<center><img src=\"https://www.htu.edu.jo/images/ThumbnailsCoverPhotos/HTU%20Logo-250px.png\" alt=\"HTU\"  width=\"180px\" align=\"center\">\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<p>\n",
    "\n",
    "**Data Mining**\n",
    "\n",
    "10204310\n",
    "\n",
    "Section (3)\n",
    "\n",
    "**Analyze and Implement text and graph mining application**\n",
    "\n",
    "**Submitted to**\n",
    "\n",
    "Eng. Bassam Kasasbeh\n",
    "\n",
    "**Submitted on**\n",
    "\n",
    "December 23rd, 2023\n",
    "\n",
    "**Submitted by**\n",
    "\n",
    "Marwan Tarek Shafiq Al Farah\n",
    "\n",
    "**Student ID**\n",
    "\n",
    "21110011\n",
    "\n",
    "Fall 2023 – 2024\n",
    "</p></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-Hj0QWQ8vkV"
   },
   "source": [
    "# **Install and Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CWKyq3owIqt0",
    "outputId": "70fd8018-73e2-47d6-a42b-9382df5893a5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\marwa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\marwa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\marwa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\marwa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances, manhattan_distances, cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from bidi.algorithm import get_display\n",
    "import arabic_reshaper\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "from PIL import Image, ImageTk\n",
    "from tkinter import ttk\n",
    "import os\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTRec2eJ80b4"
   },
   "source": [
    "# **Loading The Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'D:/Marwan/HTU/Third Year/Fall Semester/Data Mining/My Solutions/Mid/Code/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "xlzyfsO403pM"
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df_oj = pd.read_excel(path + 'Complaints dataset.xlsx', sheet_name='Complaints_v3')\n",
    "df_oj = df_oj[df_oj.columns[:-2]]\n",
    "df_oj = df_oj[df_oj.duplicated() == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "7oslAiDr6aed"
   },
   "outputs": [],
   "source": [
    "# Create a copy of the dataset\n",
    "df = df_oj.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iyR1QqQi86U2"
   },
   "source": [
    "# **Functions to Preprocess the Text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zyAaCOPeZ6JY"
   },
   "outputs": [],
   "source": [
    "# Adding spaces between each type of different characters\n",
    "def add_space_between_characters(text):\n",
    "    # Unicode range for Arabic characters\n",
    "    arabic_range = '\\u0600-\\u06FF'\n",
    "\n",
    "    # Define patterns for different types of characters\n",
    "    patterns = [\n",
    "        # English letter followed by Arabic, number, or special character\n",
    "        rf'([a-zA-Z])([\\d{arabic_range}]|[^\\w\\s])',\n",
    "        # Arabic letter followed by English, number, or special character\n",
    "        rf'([{arabic_range}])([a-zA-Z\\d]|[^\\w\\s])',\n",
    "        # Number followed by English, Arabic, or special character\n",
    "        rf'(\\d)([a-zA-Z{arabic_range}]|[^\\w\\s])',\n",
    "        # Special character followed by English, Arabic, or number\n",
    "        rf'([^\\w\\s])([a-zA-Z{arabic_range}\\d])'\n",
    "    ]\n",
    "\n",
    "    # Function to insert space between matched groups\n",
    "    def insert_space(match):\n",
    "        return ' '.join(match.groups())\n",
    "\n",
    "    # Apply each pattern\n",
    "    for pattern in patterns:\n",
    "        text = re.sub(pattern, insert_space, text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "gcDvMdUY4scT"
   },
   "outputs": [],
   "source": [
    "# Precompile Regular Expressions\n",
    "normalize_arabic_regex = re.compile(r'[إأٱآا]')\n",
    "yaa_regex = re.compile(r'[ؤئ]')\n",
    "haa_regex = re.compile(r'[ة]')\n",
    "tashkeel_regex = re.compile(r'[ًٌٍَُِّْ]')\n",
    "alnum_regex = re.compile(r'\\w')\n",
    "\n",
    "# Load Stopwords Once\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "arabic_stopwords = set(stopwords.words('arabic'))\n",
    "\n",
    "# Lemmatizer Initialization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Stemmer Initialization\n",
    "st = ISRIStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-8pVDOeK-Kgd"
   },
   "outputs": [],
   "source": [
    "# Function for POS tagging\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "wsSVV24m6oH0"
   },
   "outputs": [],
   "source": [
    "# Function to preprocess the text\n",
    "def clean_text(text, arabic_stemming = False):\n",
    "    # Normalize English text\n",
    "    text = text.lower()\n",
    "\n",
    "    # Normalize Arabic Text\n",
    "    text = normalize_arabic_regex.sub('ا', text)\n",
    "    text = yaa_regex.sub('ء', text)\n",
    "    text = haa_regex.sub('ه', text)\n",
    "    text = tashkeel_regex.sub('', text)\n",
    "\n",
    "    # Add Spaces between Arabic Letters, English Letters, Symbols, and Numbers\n",
    "    text = add_space_between_characters(text)\n",
    "\n",
    "    # Tokenize the Text\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Removing Non-Alphanumeric Characters\n",
    "    words = [word for word in words if alnum_regex.match(word) and word not in english_stopwords and word not in arabic_stopwords]\n",
    "\n",
    "    # Combined English Processing: Lemmatization and Stopwords Removal\n",
    "    words = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in nltk.pos_tag(words)]\n",
    "\n",
    "    # Arabic Preprocessing\n",
    "    words = [word[len(\"ال\"):] if word.startswith(\"ال\") else word for word in words]\n",
    "\n",
    "    # Arabic Stemming\n",
    "    if arabic_stemming:\n",
    "      words = [st.stem(word) for word in words]\n",
    "\n",
    "    # Return the Preprocessed Text\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMp9B8uJ71CX"
   },
   "source": [
    "# **Preprocess the Text & TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "avDXl1Oq1Big"
   },
   "outputs": [],
   "source": [
    "# Preprocessing the text\n",
    "df['CASE_DESC'] = df['CASE_DESC'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "usfmIfNICRLH"
   },
   "outputs": [],
   "source": [
    "# Using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_documents = vectorizer.fit_transform(df['CASE_DESC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_aLZmWlFrHHC"
   },
   "source": [
    "# **Input GUI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_dataframe(df, treeview):\n",
    "    # Clear previous data\n",
    "    for i in treeview.get_children():\n",
    "        treeview.delete(i)\n",
    "    \n",
    "    # Setting up new dataframe structure\n",
    "    treeview[\"column\"] = list(df.columns)\n",
    "    treeview[\"show\"] = \"headings\"\n",
    "    for column in treeview[\"column\"]:\n",
    "        treeview.heading(column, text=column)\n",
    "\n",
    "    # Inserting each row of data into the treeview\n",
    "    df_rows = df.to_numpy().tolist()\n",
    "    for row in df_rows:\n",
    "        treeview.insert(\"\", \"end\", values=row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def additional_action1():\n",
    "    # Create a new window\n",
    "    new_window = tk.Toplevel(root)\n",
    "    new_window.title(\"DataFrame Display\")\n",
    "\n",
    "    # Create a frame to hold the Treeview and Scrollbars\n",
    "    frame = ttk.Frame(new_window)\n",
    "    frame.pack(expand=True, fill='both')\n",
    "\n",
    "    # Create a Treeview widget in the new window\n",
    "    tree = ttk.Treeview(frame)\n",
    "\n",
    "    # Create a vertical Scrollbar and associate it with the tree\n",
    "    v_scrollbar = ttk.Scrollbar(frame, orient=\"vertical\", command=tree.yview)\n",
    "    tree.configure(yscrollcommand=v_scrollbar.set)\n",
    "\n",
    "    # Create a horizontal Scrollbar and associate it with the tree\n",
    "    h_scrollbar = ttk.Scrollbar(frame, orient=\"horizontal\", command=tree.xview)\n",
    "    tree.configure(xscrollcommand=h_scrollbar.set)\n",
    "\n",
    "    # Display the DataFrame using the previously defined function\n",
    "    display_dataframe(df_to_display, tree)\n",
    "\n",
    "    # Layout\n",
    "    tree.grid(row=0, column=0, sticky='nsew')\n",
    "    v_scrollbar.grid(row=0, column=1, sticky='ns')\n",
    "    h_scrollbar.grid(row=1, column=0, sticky='ew')\n",
    "\n",
    "    # Configure frame's row and column weights so it resizes proportionally\n",
    "    frame.grid_rowconfigure(0, weight=1)\n",
    "    frame.grid_columnconfigure(0, weight=1)\n",
    "\n",
    "\n",
    "def additional_action2():\n",
    "    # Create a new window\n",
    "    new_window = tk.Toplevel(root)\n",
    "    new_window.title(\"DataFrame Display\")\n",
    "\n",
    "    # Create a Treeview widget in the new window\n",
    "    tree = ttk.Treeview(new_window)\n",
    "\n",
    "    # Display the DataFrame using the display_dataframe function\n",
    "    display_dataframe(df_metrics.reset_index(), tree)\n",
    "\n",
    "    # Layout\n",
    "    tree.pack(expand=True, fill='both')\n",
    "\n",
    "def additional_action3():\n",
    "    # Create a new window\n",
    "    new_window = tk.Toplevel(root)\n",
    "    new_window.title(\"Directed Graph\")\n",
    "\n",
    "    # Load and display the image\n",
    "    image = Image.open(path + \"Directed Graph.png\")\n",
    "    photo = ImageTk.PhotoImage(image)\n",
    "\n",
    "    # Create a label for the text and image\n",
    "    label_text = tk.Label(new_window, text=\"Directed Graph\")\n",
    "    label = tk.Label(new_window, image=photo)\n",
    "\n",
    "    # This line is crucial to prevent the image from being garbage collected\n",
    "    label.image = photo\n",
    "\n",
    "    # Packing the label\n",
    "    label_text.pack()\n",
    "    label.pack()\n",
    "\n",
    "\n",
    "def additional_action4():\n",
    "    # Create a new window\n",
    "    new_window = tk.Toplevel(root)\n",
    "    new_window.title(\"ًWeighted Graph\")\n",
    "\n",
    "    # Load and display the image\n",
    "    image = Image.open(path + \"Weighted Graph.png\")\n",
    "    photo = ImageTk.PhotoImage(image)\n",
    "\n",
    "    # Create a label for the text and image\n",
    "    label_text = tk.Label(new_window, text=\"ًWeighted Graph\")\n",
    "    label = tk.Label(new_window, image=photo)\n",
    "\n",
    "    # This line is crucial to prevent the image from being garbage collected\n",
    "    label.image = photo\n",
    "\n",
    "    # Packing the label\n",
    "    label_text.pack()\n",
    "    label.pack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "1fQB-r3rrGRW"
   },
   "outputs": [],
   "source": [
    "# Defining the Global Variables\n",
    "df_to_display = None\n",
    "df_metrics = None\n",
    "\n",
    "# Enter the info\n",
    "def get_info():\n",
    "    # Initializing the Global Variable\n",
    "    global df_to_display, df_metrics\n",
    "    query = entry_id1.get()\n",
    "    display = None\n",
    "    distance = None\n",
    "    oj_vs_dist = None\n",
    "\n",
    "    # Choosing the number of records to display\n",
    "    try:\n",
    "        display = int(entry_id2.get())\n",
    "    except ValueError:\n",
    "        messagebox.showerror(\"Error\", \"Enter a valid integer for the number of records you want to show.\")\n",
    "        return\n",
    "\n",
    "    # Choosing the type of the distance metric\n",
    "    if entry_id3.get() not in [\"1\", \"2\", \"3\"]:\n",
    "        messagebox.showerror(\"Error\", \"Enter a number between 1 and 3 for the distance metric.\")\n",
    "        return\n",
    "    else:\n",
    "        distance = {\"1\": \"Euclidean Distance\", \"2\": \"Manhattan Distance\", \"3\": \"Cosine Similarity\"}[entry_id3.get()]\n",
    "\n",
    "    # Choosing whether to display the Distances or Original Table\n",
    "    if entry_id4.get() not in [\"1\", \"2\"]:\n",
    "        messagebox.showerror(\"Error\", \"Enter a number between 1 and 2 for the table to be displayed.\")\n",
    "        return\n",
    "    else:\n",
    "        oj_vs_dist = {\"1\": \"Distances\", \"2\": \"Original\"}[entry_id4.get()]\n",
    "\n",
    "    # Displaying all choices\n",
    "    messagebox.showinfo(\"Info\", f\"\\nYour choices:\\nQuery: {query}\\nNumber of records: {display}\\nDistance metric: {distance}\\nDisplay option: {oj_vs_dist}\")\n",
    "\n",
    "    # Preprocess and TF-IDF for the query\n",
    "    query = [clean_text(query)]\n",
    "    tfidf_query = vectorizer.transform(query)\n",
    "\n",
    "    # Calculate the Distances\n",
    "    euclidean_distance = euclidean_distances(tfidf_query, tfidf_documents)\n",
    "    normalized_euclidean_distance = (euclidean_distance - euclidean_distance.min()) / (euclidean_distance.max() - euclidean_distance.min())\n",
    "    manhattan_distance = manhattan_distances(tfidf_query, tfidf_documents)\n",
    "    normalized_manhattan_distance = (manhattan_distance - manhattan_distance.min()) / (manhattan_distance.max() - manhattan_distance.min())\n",
    "    cosine_distance = cosine_similarity(tfidf_query, tfidf_documents)\n",
    "\n",
    "    # Select case descriptions that contain at least one word from the query\n",
    "    query_indices = df[df['CASE_DESC'].apply(lambda desc: any(k in desc for k in set(k for sublist in query for k in sublist.split())))].index.tolist()\n",
    "    df_oj_results = pd.DataFrame({\"CASE_DESC\" : df_oj[\"CASE_DESC\"],\n",
    "              \"Euclidean Distance\" : normalized_euclidean_distance.flatten(),\n",
    "              \"Manhattan Distance\" : normalized_manhattan_distance.flatten(),\n",
    "              \"Cosine Similarity\" : cosine_distance.flatten()}).loc[query_indices].sort_index()\n",
    "    df_temp = df_oj_results.sort_values(by = distance, ascending = False if distance == \"Cosine Similarity\" else True)\n",
    "    df_to_display = df_oj.loc[df_temp.index].head(display) if oj_vs_dist == \"Original\" else df_temp.head(display)\n",
    "    print(df_to_display)\n",
    "\n",
    "    # Use the preprocessed case descriptions for the graphs\n",
    "    def results(distance):\n",
    "        df_results = pd.DataFrame({\"CASE_DESC\" : df[\"CASE_DESC\"],\n",
    "                                \"Euclidean Distance\" : normalized_euclidean_distance.flatten(),\n",
    "                                \"Manhattan Distance\" : normalized_manhattan_distance.flatten(),\n",
    "                                \"Cosine Similarity\" : cosine_distance.flatten()}).loc[query_indices].sort_index()\n",
    "        return df_results.sort_values(by = distance, ascending = False if distance == \"Cosine Similarity\" else True)\n",
    "    \n",
    "    # Directed Graph\n",
    "    top_case_desc = [\" \".join([get_display(arabic_reshaper.reshape(j)) for j in word_tokenize(i)]) for i in list(results(distance)['CASE_DESC'].head(display))]\n",
    "    def remove_duplicates(sentence):\n",
    "        # Split the sentence into words\n",
    "        words = sentence.split()\n",
    "\n",
    "        # Use an ordered dictionary to remove duplicates while preserving order\n",
    "        from collections import OrderedDict\n",
    "        unique_words = OrderedDict.fromkeys(words)\n",
    "\n",
    "        # Join the unique words back into a sentence\n",
    "        return ' '.join(unique_words)\n",
    "    top_case_desc = [remove_duplicates(i) for i in top_case_desc]\n",
    "    names_dict = {get_display(arabic_reshaper.reshape(j)): j for i in list(results(distance)['CASE_DESC'].head(display)) for j in word_tokenize(i)}\n",
    "    nodes = list(set(word_tokenize(\" \".join(top_case_desc))))\n",
    "    edges_directed = [item for sublist in [[(i[j], i[j+1]) for j in range(len(i) - 1)] for i in [word_tokenize(i) for i in top_case_desc]] for item in sublist]\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from(nodes)\n",
    "    G.add_edges_from(edges_directed)\n",
    "    pos = nx.circular_layout(G)\n",
    "    nx.draw(G, pos, with_labels = True, arrows = True, node_size = 1000, node_color = 'red', font_color = 'white', arrowsize = 10, font_size = 10)\n",
    "    plt.savefig(\"Directed Graph.png\", format='PNG')\n",
    "    plt.show(block=False)\n",
    "    df_metrics = pd.DataFrame({\"Page Rank\":nx.pagerank(G),\n",
    "                           \"Degree Centrality\":nx.degree_centrality(G),\n",
    "                           \"Betweenness Centrality\":nx.betweenness_centrality(G),\n",
    "                           \"Closeness Centrality\":nx.closeness_centrality(G)})\n",
    "    df_metrics = df_metrics.rename(index=names_dict)\n",
    "\n",
    "    # Weighted Graph\n",
    "    temp_list = [(word, k) for i in top_case_desc for j, word in enumerate(word_tokenize(i)) for k in word_tokenize(i)[j + 1:]]\n",
    "    for i in [(nodes[i], nodes[j]) for i in range(len(nodes) - 1) for j in range(i + 1, len(nodes))]:\n",
    "        temp_list.append(i)\n",
    "    edges_weighted = [(key[0], key[1], value-1) for key, value in dict(pd.Series([tuple(sorted(i)) for i in temp_list]).value_counts()).items()]\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(nodes)\n",
    "    G.add_weighted_edges_from(edges_weighted)\n",
    "    pos = nx.circular_layout(G)\n",
    "    nx.draw(G, pos, with_labels = True, arrows = True, node_size = 1000, node_color = 'red', font_color = 'white', font_size = 10)\n",
    "    nx.draw_networkx_edge_labels(G, pos, nx.get_edge_attributes(G, \"weight\"))\n",
    "    plt.savefig(\"Weighted Graph.png\", format='PNG')\n",
    "    plt.show(block=False)\n",
    "\n",
    "    # Adding the 4 buttons\n",
    "    btn_additional1.pack()\n",
    "    btn_additional2.pack()\n",
    "    btn_additional3.pack()\n",
    "    btn_additional4.pack()\n",
    "\n",
    "# Main window\n",
    "root = tk.Tk()\n",
    "root.title(\"Query System\")\n",
    "\n",
    "# Widgets\n",
    "label_id1 = tk.Label(root, text=\"Please enter your search query:\")\n",
    "entry_id1 = tk.Entry(root)\n",
    "label_id2 = tk.Label(root, text=\"\\nEnter the number of records you want to display:\")\n",
    "entry_id2 = tk.Entry(root)\n",
    "label_id3_0 = tk.Label(root, text=\"\\nSelect a distance metric for sorting the records:\")\n",
    "label_id3_1 = tk.Label(root, text=\"1. Euclidean Distance\")\n",
    "label_id3_2 = tk.Label(root, text=\"2. Manhattan Distance\")\n",
    "label_id3_3 = tk.Label(root, text=\"3. Cosine Similarity\")\n",
    "label_id3_4 = tk.Label(root, text=\"Enter the number of your choice:\")\n",
    "entry_id3 = tk.Entry(root)\n",
    "label_id4_0 = tk.Label(root, text=\"\\nWould you like to display:\")\n",
    "label_id4_1 = tk.Label(root, text=\"1. The distances table sorted by the chosen distance\")\n",
    "label_id4_2 = tk.Label(root, text=\"2. The original dataset sorted by the chosen distance\")\n",
    "label_id4_3 = tk.Label(root, text=\"Enter the number of your choice:\")\n",
    "entry_id4 = tk.Entry(root)\n",
    "btn_get_info = tk.Button(root, text=\"Submit\", command=get_info)\n",
    "info_text = tk.Label(root, text=\"\", font=(\"Helvetica\", 12))\n",
    "\n",
    "# Packing all elements\n",
    "label_id1.pack()\n",
    "entry_id1.pack()\n",
    "label_id2.pack()\n",
    "entry_id2.pack()\n",
    "label_id3_0.pack()\n",
    "label_id3_1.pack()\n",
    "label_id3_2.pack()\n",
    "label_id3_3.pack()\n",
    "label_id3_4.pack()\n",
    "entry_id3.pack()\n",
    "label_id4_0.pack()\n",
    "label_id4_1.pack()\n",
    "label_id4_2.pack()\n",
    "label_id4_3.pack()\n",
    "entry_id4.pack()\n",
    "btn_get_info.pack()\n",
    "info_text.pack()\n",
    "\n",
    "# Additional Buttons Definition\n",
    "btn_additional1 = tk.Button(root, text=\"Display DataFrame\", command=additional_action1)\n",
    "btn_additional2 = tk.Button(root, text=\"Display Centrality Measures\", command=additional_action2)\n",
    "btn_additional3 = tk.Button(root, text=\"Display Directed Graph\", command=additional_action3)\n",
    "btn_additional4 = tk.Button(root, text=\"Display Weighted Graph\", command=additional_action4)\n",
    "\n",
    "# Set the additional buttons to be initially hidden\n",
    "btn_additional1.pack_forget()\n",
    "btn_additional2.pack_forget()\n",
    "btn_additional3.pack_forget()\n",
    "btn_additional4.pack_forget()\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "l-Hj0QWQ8vkV",
    "lTRec2eJ80b4",
    "iyR1QqQi86U2",
    "GMp9B8uJ71CX",
    "_aLZmWlFrHHC",
    "Tu63AXgi75TB",
    "LWl71WWv8Bhn",
    "43J4PI4F8EPP",
    "Qn18M7n28HAc",
    "UkxfifT_8Op5",
    "RyJMGz-I8Wi3",
    "niV7W_GMvQkV",
    "2t_kgJeIvSp7",
    "iKQyBd0d1nBd",
    "yoa_joqvvYHt",
    "FAbCzH8Uvbnj",
    "DHrMRRx4viWt",
    "rHY0OZSGvmSM",
    "dDodVxUOvo8h"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
